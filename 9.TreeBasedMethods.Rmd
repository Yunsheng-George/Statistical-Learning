---
title: "TreeBasedMethods"
author: "Yunsheng Lu"
date: "2023-01-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In general, the advantage for tree-based methods is due to its simple and clear interpretation, although the performance sometimes might not be as good as some of the models we discussed before.

# Decision Trees

### Regression Trees

The fundamental model for regression trees is a binary splitting tree, as illustrated in the following picture:
![Alt text](/Users/luyunsheng/Desktop/Winter Stat ML/RegressionTree.png)

We process the regression tree in two steps:

1. Divide the set of all possible values of $X_1,X_2,\dots,X_n$ into non-overlapping boxes (binary splitting) $R_1,\dots,R_j$

2. For every observation that falls into the region $R_j$, we consider $\sum_{i \in R_j} (y_i-\hat{y}_i)^2$, where $\hat{y}_i$, the estimated value for the region $R_j$ is the average of the point in the sampling.

3. We split in the way that $\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_i)^2$

How to pick the proper splitting value and the predictor? It's computationally formidable if we take every subtree into consideration. Here we use the greedy approach: each time when we split the region, we only consider the current besting splitting. More specifically, if We are at the region $R_j$ and consider splitting it into $R_1$ and $R_2$. We consider each possible choice of $X_j$ and $s \in \mathbb{R}$ and let $R_1=\{X|X_j<s\}$ and,$R_2=\{X|X_j \geq s\}$. We are trying to find the best combination of (j,s) so that $\sum_{i:x_i \in R_1(j,s)} (y_i-\hat{y}_i)^2+\sum_{i:x_i \in R_2(j,s)} (y_i-\hat{y}_i)^2$ is minimized.

Once we divide $R_j$  into $R_1$ and $R_2$, we again use the same algorithm to further divide $R_1$ and $R_2$ recursively.

As we said, one advantage of using tree based method is because it's easy to interpret. However, if have too complicated regions divided, it's still not practical for us to interpret the model. Thus this reminds us of "the trade-off between interpretation and complexity". 

What if we further divide the tree only if thid further division will lead to a reduction in MSE above a (high threshold)? One limitation of this approcah is that we don't know if our current seemingly meaningless division will lead to a subsequent meaningful division. Thus, what we do is not to grow a small tree, but prune a big tree into a small tree.

We consider to minimize: $\sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i-\hat{y}_{R_m})^2+\alpha|T|$ Here $T$ represents the terminal nodes, which are different regions splitted by the tree, so |T| is the total number of areas. $\alpha$ here is the tuning parameters, which control the balance between the goodness of fit and the complexity of the model. This expression is called "Cost Complexity Pruning".

However, what's the best $\alpha$? Like what we did before, the tuning parameter is always selected by the Cross-Validation Approach.

In fact, we can view regression tree as a special linear model: $f(X)=\sum_{m=1}^M c_mI_{X \in R_m}$, but with a different fitting method (not minimizing least squres),
```{r}
#!!Almost all the methods related to trees are in library(tree)
library(tree)
library(ISLR2)
attach(Boston)
set.seed(1)
train<-sample(1:nrow(Boston),nrow(Boston)/2)
#tree( ) perform regression/classification tree method, which is quite similar to lm()
tree.boston<-tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston<-cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type="b")
#prune the tree to be 5-nodes
prune.boston<-prune.tree(tree.boston,best=5)
plot(tree.boston)
text(tree.boston,pretty=0)
yhat<-predict(tree.boston,newdata=Boston[-train,])
boston.test<-Boston[-train,"medv"]
plot(yhat,boston.test)
#MSE of the pruned tree
mean((yhat-boston.test)^2)
```


### Classification Trees
Similar to the case of linear regressions, classification trees are similar to regression trees except that it's dealing with the problem of qualitative responses. To measure the performance of the tree at region $R_m$, we use the Classification Error Rate: $E=1-\max_{k} \hat{p}_{mk}$. However, in problems related to classifications, Classification Purity is a better measure than Classification Error Rate. There are two indices related to model purity:

1. Gini Index: $G=\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$

2. Entrophy: $D=-\sum_{k=1}^K \hat{p}_{mk} \log(\hat{p}_{mk})$ 

In fact, these two indices providing numerically similar results.

```{r}
attach(Carseats)
#Use ifelse( ) function to create a variable called High, which creates a variable taking on value NO if Sales <= 8. Ifelse( ) is very useful in the case of classification
High<-factor(ifelse(Sales<=8,"No","Yes"))
#data.frame( ) to merge the data
Carseats<-data.frame(Carseats,High)
tree.carseats<-tree(High~.-Sales,Carseats)
summary(tree.carseats)
```

In the summary above, the deviance is given by $-2\sum_m\sum_kn_{mk}\log(\hat{p_{mk}})$, which is closely related to entrophy.The residual mean deviance is given by the deviance divided by $n-|T_0|$, where $T_0$ denotes the set of all terminal nodes. In our case, it's 400-27=373.

```{r}
plot(tree.carseats)
#text() displays the node labels, while pretty=0 means the R including the category names for any qualitive variables, instead of displaying a letter for each category
text(tree.carseats,pretty=0)
#directly typing tree.carseats shows the text version of the tree structure.
tree.carseats
```

Next we see the performance of classification tree:
```{r}
set.seed(2)
train<-sample(1:nrow(Carseats),200)
Carseats.test<-Carseats[-train,]
tree.carseats<-tree(High~.-Sales,Carseats,subset=train)
#To predict values related to decision treem we need to set type="class"
tree.pred<-predict(tree.carseats,Carseats.test,type="class")
High.test<-High[-train]
table(tree.pred,High.test)
mean(tree.pred==High.test)
```

We will see if pruning the tree leads to better result:
```{r}
set.seed(7)
#cv.tree( ) provides the cross-validation approach on whether the complexity of tree is optimized
#FUN=prune.misclass means we want classification error rate to be the guidance for our CV, instead of using the "default guidance", deviance
cv.carseats<-cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b") # k here the tuning parameter alpha
#We can also prune the tree with the size we want
prune.carseats<-prune.misclass(tree.carseats,best=9) #here we want the result be 9 nodes
text(prune.carseats,pretty=0)
#Compare pruned tree and the original tree
tree.pred<-predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
mean(tree.pred==High.test)
```



# Bagging
Bagging is also called Bootstrap Aggregation. Basically, it's a way to improve the performance using decision trees by averaging the results of different models based on different datasets generated by bootstrap. Suppose we generate B of bootstrap samples,

for regression trees: $\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x)$. Here $\hat{f}^{*b}(x)$n denotes to be the result from $b^{th}$ bootstrap data.

for classification trees: $\hat{f}_{bag}(x)=\max_{k \in K} \hat{f}^{*b}(x)$, where $K$ denotes to be the set of all responses.

```{r}
attach(Boston)
#We use randomForest library for to perform both random forest and bagging
library(randomForest)
set.seed(1)
train<-sample(1:nrow(Boston),nrow(Boston)/2)
#mtry=12 indicates that all 12 predictors are considered for each split of the tree
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=12,importance=TRUE)
bag.boston
#check the performance of this model
yhat.bag<-predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag,boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
#We change the number of trees grown using ntree=
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=12,ntree=25)
yhat.bag<-predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
importance(bag.boston)
```


### Out-of-Bag Error Estimation
Question: how do we measure the variance from Bagging? In fact, besides Cross-Validation approach, there is a better way of doing this, although essentially speaking, they have the same logic. 

Recall that the prediction variance should be estimated by data outside of the training dataset. What are the most accessible data out of the training dataset? Roughly speaking, each bootstrapped dataset will contain 2/3 of total observations in the original data set (this is an easy combinatorical problem). The following picture gives an intuition:

![Alt text](/Users/luyunsheng/Desktop/Winter Stat ML/OOB.png)

Then There are 1/3 of observations out of the bootstrapped dataset, which is referred to as "OOB". For each OOB corresponding to each bootstrapped dataset, we will have one model and for each observation point, that model has a prediction for that point, so there is a total of B/3 of prediction value for each of the n observation points, and we can calculate the overall (average) OOB value and its variance for each observation point. 

Limit: However, bagging is not so good for interpretation, but anyway, it can provdie an overall description of the importance of each variables, which is more accurate than simple decision trees.

# Random Forests

This is similar to Bagging, but solves one difficult the Bagging method is faced to. There might be some very strong predictors, and for each tree, the top splitting will always choose that predictors and make all the trees very similar, thus highly correlated. To avoid this problem, we have the Random Forests method. At each split, we randomly choose $m=\sqrt{p}$ number of total p predictors as our candidates. In this way, we avoid to repeatedly using the same strong predictor and thus decorrelate the trees.

```{r}
#In fact, in previous block we have already perform "semi-random forest". For the real RF, we typically use m=p^0.5, but here we use m=6
set.seed(1)
rf.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf<-predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
#Use importance( ) we can view the importance of each predictor
importance(rf.boston)
#For each variable, IncMSE shows the amount of increment in MSE if we change this particular predictor to another one; IncNodePurity shows the amount of change in node purity before before and after the splits over that predictor.
plot(rf.boston)
```

# Boosting

Boosting is similar to bagging, but the new trees are built based on the old trees. Basically speaking, instead of growing a tree based on the observations $y$, we grow the new tree on the residual of the previous tree. 

The Algorithm is designed below:

1. set $\hat{f}(x)=0$ and $r_i=y_i$ for all $i$ in the training set.

2. For b=1,2,3,...,B, repeat:

(a) Fit a tree $\hat{f}^b$ with d splits (d+1 terminal nodes) to the training data (X,r), where X is the observation matrix, and r is the residual vector.

(b) Update fitting model:$\hat{f}(x) \leftarrow \hat{f}(x)+\lambda\hat{f}^b(x)$

(c) Update residuals: $r_i \leftarrow  r_i-\lambda\hat{f}^b(x)$

3. Output the boosted model: $\hat{f}(x)=\sum_{b=1}^B \lambda\hat{f}^b(x)$

$\lambda$ here is the tuning parameter control the speed of the model's learning

```{r}
#We use gbm( ) in gbm library to perform Boosting
library(gbm)
set.seed(1)
#distribution="Gaussian" indicates the response value is normally distributed
#n.trees denotes the total number of trees wanted to be generated
#interaction.depth gives an upper bound for the depth of each tree
boost.boston<-gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
#Notice the summary will directly give a plot
summary(boost.boston)
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost<-predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
#We can change the tuning parameter lambda,using shrinkage
boost.boston<-gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=T)
summary(boost.boston)
boost.boston<-gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=T)
summary(boost.boston)
yhat.boost<-predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
#By demonstration, verbose=T will record and print the whole progress for the whole procedure.
mean((yhat.boost-boston.test)^2)
```

# Bayesian Additive Regression Trees

BART can be viewed as the combination of bagging and boosting. In general, we have B turns of iterations. In each iteration, we grow K number of new trees based on the trees grown in the previous iterations. Thus, the iteration can be viewed as boosting but within each iteration, it can be viewed as bagging. The following displays the algorithm: (note thta $\hat{f}^b_k(x)$ here denotes to be the predicted value for $k^{th}$ tree in $b^{th}$ iteration)

1. Let $\hat{f}^1_1(x) = \hat{f}^1_2(x) = \dots = \hat{f}^1_K(x) = \frac{1}{nK} \sum_{i=1}^n y_i$

2. Compute $f^1(x)=\sum_{k=1}^K \hat{f}_k^1 (x)=\frac{1}{n}\sum_{i=1}^n y_i$

3. For b=2,..., B:

(a) For k=1,...,K:

(a1) For i=1,...,n, compute the current partial residual: $r_i=y_i-\sum_{k'<k} \hat{f}_{k'}^{b}(x_i)-\sum_{k'>k}\hat{f}_{k'}^{b-1}(x_i)$

(a2) Fit a new tree, $\hat{f}_k^b(x)$, to $r_i$, by randomly perturbing the kth tree from the previous iteration, $\hat{f}_k^{b-1}(x)$. Pertubations that improve the fit are favored. (The detailed choice for perturbation is described as the following picture)

(b) Compute $\hat{f}^b(x)=\sum_{k=1}^K \hat{f}_k^b(x)

4. Compute the mean after L burn-in samples: $\hat{f}(x)=\frac{1}{B-L}\sum_{b=L+1}^B\hat{f}^b(x)$. We abandoned the first $L$ trees because they are relatively bad fitting than the trees grown later.

![Alt text](/Users/luyunsheng/Desktop/Winter Stat ML/perturbation.png)
```{r}
library(BART)
x<-Boston[,1:12]
y<-Boston[,"medv"]
xtrain<-x[train,]
ytrain<-y[train]
xtest<-x[-train,]
ytest<-y[-train]
set.seed(1)
bartfit<-gbart(xtrain,ytrain,x.test=xtest)
summary(bartfit)
bartfit$varcount.mean
yhat.bart<-bartfit$yhat.test.mean
mean((ytest-yhat.bart)^2) #We can see the MSE is lower here
#check how many times each variable appeared
ord<-order(bartfit$varcount.mean,decreasing=T)
ord
bartfit$varcount.mean[ord]
```





